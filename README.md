# Estimating the distribution of numerosity and non-numerical visual magnitudes in natural scenes using computer vision
This repository is the **original implementation** of the [paper](https://arxiv.org/abs/2409.11028):  _Estimating the distribution of numerosity and non-numerical visual magnitudes in natural scenes using computer vision_ 

With this repostitory, we propose an innovative computer vision pipeline that can be used to automatically process naturalistic images in order to identify and locate multiple objects in that image and provide a precise segmentation of their silhouette.
## Installation
1. Install G-DINO \
Clone Grounding DINO\
`git clone https://github.com/IDEA-Research/GroundingDINO.git`\
Make sure you have set CUDA_HOME as your environment variable. \
On Linux:
`export CUDA_HOME=/path/to/cuda` \
On Windows:
`setx CUDA_HOME=/path/to/cuda` \
**NB**: only complete version of CUDA will work, simply install cudatoolkit or nvcc from conda or other source is not possible \
Now install by the following commands: \
`cd GroundingDINO/` \
`pip install -e .` 

2. Install SAM \
Clone segment anything\
`git clone https://github.com/facebookresearch/segment-anything.git`\
Install with the following commands\
`python -m pip install -e segment_anything` \
Download SAM checkpoints \
`wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth`

3. Setup Google Vertex AI API\
If you wish to have your own results from GEMINI, setup [Google cloud API](https://cloud.google.com/vertex-ai/docs/start/cloud-environment?_gl=1*1duy806*_ga*ODA4NTY5MTUzLjE2ODc2ODMyODQ.*_ga_WH2QY8WWF5*MTcyNTcxNzY0MS4xMS4xLjE3MjU3MTc2NTcuNjAuMC4w) as instructed.\
**NB**: Any other multimodal large language models can be used.

4. Install other requirements \
`pip install requirements.txt`

## How to use
With this repository, you can start from the very beginning of our pipeline, that is, to generate responses from Gemini. Meanwhile, you can also start at any middle steps as we provide all the results for each step. Here is a step by step overview of how the data files are produced.

1. Prompt Gemini with your desire dataset. 
2. Post process the responses of Gemini by running *ProcessingPrompts.py* 
3. Prompt Grounding DINO with the images and their corresponding textual prompts (i.e. the objects identified by Gemini)
4. Filter the bounding boxes with your own logits (check our paper for more details)
5. Prompt SAM with the images and the coordinates of the bounding boxes generated by Grounding DINO

If you wish to start from the middle step or using the data from the paper, do following:
1. Create a "Data" folder to store our data\
`mkdir Data` 
2. [Download](https://unipdit-my.sharepoint.com/:u:/g/personal/kuinan_hou_studenti_unipd_it/EQ2w1OnZejdAkq7qtLZs8pEB3EgOMy3pr2GVy05FJOyrFQ?e=5AvNJH) our data and unzip them into the folder you just created

## Data structure
Responses from each model are stored in python dictionaries. The files are named by model_result_dataset.pkl or .json\
One general rule used for all dictionaries is the keys of all dictionariesis are the image IDs. The image IDs of MSCOCO can either be int or str, run dict.keys() to verify before processing.
Other information can be found in the scripts.

## Acknowlegdgements
[Grounding DINO](https://github.com/IDEA-Research/GroundingDINO?tab=readme-ov-file) \
[Segment Anything](https://github.com/facebookresearch/segment-anything)

## Citation
If you find this repository helpful, please cite us! 
```bibtex
@misc{hou2024estimatingdistributionnumerositynonnumerical,
      title={Estimating the distribution of numerosity and non-numerical visual magnitudes in natural scenes using computer vision}, 
      author={Kuinan Hou and Marco Zorzi and Alberto Testolin},
      year={2024},
      eprint={2409.11028},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2409.11028}, 
}
```
Other citations
```bibtex
@article{liu2023grounding,
  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},
  journal={arXiv preprint arXiv:2303.05499},
  year={2023}
}

@article{kirillov2023segany,
  title={Segment Anything}, 
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={arXiv:2304.02643},
  year={2023}}